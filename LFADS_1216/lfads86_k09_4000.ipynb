{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to reproduce an example from the paper:\n",
    "\n",
    "[Inferring single-trial neural population dynamics using sequential auto-encoders](https://www.nature.com/articles/s41592-018-0109-9)\n",
    "\n",
    "We will generate trajectories from a latent Lorentz attractor and apply LFADS to recover the latent dynamic. Instead of using the original code (https://lfads.github.io/lfads-run-manager/) we will use tensorflow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6058a8fc54c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mphysical_devices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "import funs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from latent Lorenz attractor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = 20  # 50 worse\n",
    "neuron_num = 63\n",
    "factor_dim = 20\n",
    "all_time =5000\n",
    "datax = scipy.io.loadmat(\"spike63_k09_5000.mat\")\n",
    "y_data = datax[\"spike63_k09_5000\"].T\n",
    "traj = scipy.io.loadmat(\"trj63_k09_5000.mat\")\n",
    "traj = traj[\"trj63_k09_5000\"].T\n",
    "print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRep=int(all_time/Time)\n",
    "output = np.zeros((NRep,Time,neuron_num))\n",
    "\n",
    "for i in range(NRep):\n",
    "    output[i,:,:] = y_data[Time*i:Time*(i+1),:]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(traj[:,0],traj[:,1])\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax1 = plt.subplot(311)\n",
    "ax1.plot(traj[:, 0], lw=4, color='k')\n",
    "\n",
    "ax2 = plt.subplot(312, sharex=ax1)\n",
    "ax2.plot(traj[:, 1], lw=4, color='k')    \n",
    "output = output.astype('float32')\n",
    "\n",
    "# Z score output:\n",
    "output=output.reshape(Time*NRep,neuron_num)\n",
    "from scipy import stats\n",
    "output = stats.zscore(output,axis=0)\n",
    "output = output.reshape(NRep,Time,neuron_num)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(neuron_num):\n",
    "    fig, ax = plt.subplots()\n",
    "    mappable = ax.scatter(traj[:,0], traj[:,1], c=y_data[:,i] ,cmap='coolwarm',vmin=0,vmax=8)\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(output[0,:,:].T,aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('neuron')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((output))\n",
    "#print(dataset)\n",
    "#dataset = dataset.shuffle(20, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Maps Trajectory to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_dim=64,\n",
    "                 num_gru_unit=64,\n",
    "                 sequence_length=Time,\n",
    "                 num_input_neuron=neuron_num,\n",
    "                 name='encoder',\n",
    "                 **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)        \n",
    "        self.bidir_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(num_gru_unit,return_sequences=False), \n",
    "                                                       input_shape=(sequence_length, num_input_neuron))\n",
    "        self.dense_mean = tf.keras.layers.Dense(latent_dim)\n",
    "        self.dense_log_var = tf.keras.layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.bidir_gru(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_dim=64,\n",
    "                 sequence_length=Time,\n",
    "                 num_input_neuron=neuron_num,\n",
    "                 num_factor=factor_dim,\n",
    "                 name='decoder',\n",
    "                 **kwargs):\n",
    "        self.sequence_length = sequence_length\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.recurrent_decoder = tf.keras.layers.GRU(latent_dim,return_sequences=True)#,use_bias=False)\n",
    "        self.linear_bottleneck = tf.keras.layers.Dense(num_factor, activation=None)\n",
    "        self.dense_output = tf.keras.layers.Dense(512, activation='tanh')\n",
    "        self.dense_output2 = tf.keras.layers.Dense(num_input_neuron, activation=None)\n",
    "        self.latent_dim=latent_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        null_input = tf.zeros([batch_size,self.sequence_length,self.latent_dim])\n",
    "        decoder_output = self.recurrent_decoder(null_input, initial_state=inputs)\n",
    "        x = self.linear_bottleneck(decoder_output)\n",
    "        x2 = self.dense_output(x)\n",
    "        xo = self.dense_output2(x2)\n",
    "        #xo = tf.exp(x2)\n",
    "        return xo, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_dim=64,\n",
    "                 num_gru_unit=64,\n",
    "                 sequence_length=Time,\n",
    "                 num_input_neuron=neuron_num,\n",
    "                 num_factor=factor_dim,\n",
    "                 name='autoencoder',\n",
    "                 **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.encoder = Encoder(latent_dim=latent_dim,num_gru_unit=num_gru_unit,sequence_length=sequence_length,\n",
    "                               num_input_neuron=num_input_neuron)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, sequence_length=sequence_length,\n",
    "                              num_input_neuron=num_input_neuron,num_factor=num_factor)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)[0]\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    def latent(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        latent = self.decoder(z)[1]\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "batched_dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(latent_dim=64,\n",
    "                             num_gru_unit=32,\n",
    "                             sequence_length=Time,\n",
    "                             num_input_neuron=neuron_num,\n",
    "                             num_factor=factor_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=7e-4,clipvalue=200)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearly scheduling on the Kullback–Leibler divergence penalty:\n",
    "\n",
    "We use a linearly increasing schedule on the Kullback–Leibler divergence penalty (as suggested [here](https://arxiv.org/pdf/1511.06349.pdf)) so that the optimization does not quickly (and pathologically) set the Kullback–Leibler divergence to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_penalty = np.linspace(0,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for i,kl_ in enumerate(KL_penalty):\n",
    "    print(kl_,i)\n",
    "    if i > 5:\n",
    "        break\n",
    "    # Iterate over epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,),'i',i)\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, x_batch_train in enumerate(batched_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                reconstructed = vae(x_batch_train)\n",
    "                # Compute reconstruction loss\n",
    "                loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "                #print(\"loss\",loss,\"kl\",sum(vae.losses))\n",
    "                loss += kl_*sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "            grads = tape.gradient(loss, vae.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "            loss_metric(loss)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('step %s: mean loss = %s' % (step, loss_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qy_mean_est = np.zeros((all_time,neuron_num))\n",
    "for step, x_batch_train in enumerate(batched_dataset):\n",
    "    qy_mean_est[Time*step:Time*(step+1),:] = vae(x_batch_train)\n",
    "for i in range(neuron_num):\n",
    "    plt.figure()\n",
    "    plt.ylim(-2,20)\n",
    "    plt.plot(y_data[:,i],\"k\",linewidth = 2)\n",
    "    plt.plot(qy_mean_est[:,i],\"r\",linewidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz_mean_est = np.zeros((all_time,factor_dim))\n",
    "for step, x_batch_train in enumerate(batched_dataset):\n",
    "    qz_mean_est[Time*step:Time*(step+1),:] = vae.latent(x_batch_train)\n",
    "wgt = np.linalg.lstsq(qz_mean_est-qz_mean_est.mean(), traj-traj.mean())[0] \n",
    "qz_mean_est = np.dot(qz_mean_est,wgt)\n",
    "qz_est_norm = np.stack(qz_mean_est)/np.linalg.norm(np.stack(qz_mean_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Quality of reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(qz_mean_est[:,0],qz_mean_est[:,1])\n",
    "#qz_est_c = qz_mean_est - qz_mean_est.mean(axis=0)\n",
    "#qz_est_norm = qz_est_c/np.linalg.norm(qz_est_c)\n",
    "#qz_est_norm = np.stack(qz_mean_est)/np.linalg.norm(np.stack(qz_mean_est))\n",
    "\n",
    "z_true_c = traj - traj.mean(axis=0)\n",
    "z_true_norm = z_true_c/np.linalg.norm(z_true_c)\n",
    "R = funs.compute_optimal_rotation(np.stack(qz_est_norm), z_true_norm, scale=True)\n",
    "qz_est_norm_R = np.stack(qz_est_norm).dot(R)\n",
    "\n",
    "from scipy import signal\n",
    "qz_est_norm_R[:,0] = signal.savgol_filter(qz_est_norm_R[:,0], 51, 5)\n",
    "qz_est_norm_R[:,1] = signal.savgol_filter(qz_est_norm_R[:,1],51, 5)\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "plt.subplot(211)\n",
    "plt_post = plt.plot(qz_est_norm_R[:,0],'r', linewidth = 3, label = 'posterior mean')\n",
    "plt_true = plt.plot(z_true_norm[:,0], 'k', linewidth = 3, label = '\\\"true\\\" mean')\n",
    "plt.subplot(212)\n",
    "plt_post = plt.plot(qz_est_norm_R[:,1],'r', linewidth = 3, label = 'posterior mean')\n",
    "plt_true = plt.plot(z_true_norm[:,1], 'k', linewidth = 3, label = '\\\"true\\\" mean')\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.plot(z_true_norm[:,0], z_true_norm[:,1], lw=2, color = 'k')\n",
    "plt.plot(qz_est_norm_R[:,0], qz_est_norm_R[:,1], lw=2, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36tg20)",
   "language": "python",
   "name": "conda_py36tg20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
